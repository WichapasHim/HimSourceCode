CREATE DATABASE accenture_test;


CREATE TABLE ITEM(
	ITEM_ID INT NOT NULL PRIMARY KEY,
	ITEM_NAME VARCHAR(255),
	IS_ACTIVE BIT
	);


CREATE TABLE ITEM(
	ITEM_ID INT NOT NULL PRIMARY KEY,
	ITEM_NAME VARCHAR(255),
	IS_ACTIVE BIT
	);



CREATE TABLE ITEM_PRICE(
	ITEM_PRICE_ID INT NOT NULL PRIMARY KEY,
	ITEM_ID  INT FOREIGN KEY REFERENCES ITEM(ITEM_ID),
	PRICE INT,
	EFFECTIVE_DATE DATE,
	IS_ACTIVE BIT
	);


INSERT INTO ITEM(ITEM_ID,ITEM_NAME,IS_ACTIVE)
VALUES 
   (1,'7” MATERIAL-A',1),
   (2,'12” MATERIAL-A',1),
   (3,'9” MATERIAL-B',1),
   (4,'20” MATERIAL-B',0),
   (5,'MATERIAL-C',1),
   (6,'MATERIAL-D',1);


INSERT INTO ITEM_PRICE(ITEM_PRICE_ID,ITEM_ID,PRICE,EFFECTIVE_DATE,IS_ACTIVE)
VALUES 
   (1,1,100,'2020-01-01',1),
   (2,1,120,'2020-07-01',0),
   (3,1,140,'2021-01-01',1),
   (4,1,160,'2021-07-01',1),
   (5,2,60,'2020-01-01',1),
   (6,2,80,'2020-07-01',1),
   (7,2,100,'2021-01-01',1),
   (8,3,10,'2020-01-01',1),
   (9,3,20,'2020-07-01',1),
   (10,4,200,'2020-01-01',1),
   (11,5,100,'2020-01-01',0),
   (12,5,200,'2021-01-01',0),
   (13,6,300,'2021-01-01',1);



bf







Structure Data can ingest to ADF directly(Remark* after set up credential,and connect API from Sharepoint)
Then data will be ingested into datalake (datalake can store any type of data with cost saving) next step Databricks is going to take action on transformation process
(Databricks is a distributed system based on Spark which is very useful when encounter with BigData) after transform data(Processed) will move to Datawarehouse then Power BI will take care of it


UnStructureData first it move to Event hub(Event Hubs is a fully managed, real-time data ingestion service which is mostly an UnStructureData like IoT device etc.) then  Data will be seperated in two way  first(Batch) ingest to store in datalake through Azure function(Azure Functions is a cloud service available on-demand that provides all the continually updated infrastructure and resources needed to run your applications. You focus on the pieces of code that matter most to you, and Functions handles the rest. Functions provides serverless compute for Azure. You can use Functions to build web APIs, respond to database changes, process IoT streams, manage message queues, and more) and go to databricks to do transformation and so on (same as structure)
second(Streaming data) move to Azure streaming analystics(Azure Stream Analytics is a real-time analytics and complex event-processing engine that is designed to analyze and process high volumes of fast streaming data from multiple sources simultaneously) and go to Machine learning to do more complex computation or go to Databricks and end with PowerBI same as Structure data

Remark* Azure Data Catalog is a fully managed cloud service. It lets users discover the data sources they need and understand the data sources they find. At the same time, Data Catalog helps organizations get more value from their existing investments.
